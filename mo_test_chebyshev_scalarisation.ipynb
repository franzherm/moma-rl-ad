{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import mo_gymnasium as mo_gym\n",
    "import numpy as np\n",
    "from src import MO_DQN\n",
    "from src.utils import ChebyshevScalarisation\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Scalarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iterations: 100%|██████████| 200000/200000 [29:06<00:00, 114.53it/s]\n",
      "/home/franz/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.config to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.config` for environment variables or `env.get_wrapper_attr('config')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "Weight tuple: 100%|██████████| 66/66 [03:58<00:00,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     repetition_number  weight_index weight_tuple  normalised_speed_reward  \\\n",
      "0                    0             0   [0.0, 1.0]                 0.003333   \n",
      "1                    1             0   [0.0, 1.0]                 0.003333   \n",
      "2                    2             0   [0.0, 1.0]                 0.003333   \n",
      "3                    3             0   [0.0, 1.0]                 0.003333   \n",
      "4                    4             0   [0.0, 1.0]                 0.003333   \n",
      "..                 ...           ...          ...                      ...   \n",
      "325                  0            65   [1.0, 0.0]                 0.666667   \n",
      "326                  1            65   [1.0, 0.0]                 0.490000   \n",
      "327                  2            65   [1.0, 0.0]                 0.666667   \n",
      "328                  3            65   [1.0, 0.0]                 0.130000   \n",
      "329                  4            65   [1.0, 0.0]                 0.176667   \n",
      "\n",
      "     normalised_energy_reward  raw_speed_reward  raw_energy_reward  \n",
      "0                    1.000000          0.333333         100.000000  \n",
      "1                    1.000000          0.333333         100.000000  \n",
      "2                    1.000000          0.333333         100.000000  \n",
      "3                    1.000000          0.333333         100.000000  \n",
      "4                    1.000000          0.333333         100.000000  \n",
      "..                        ...               ...                ...  \n",
      "325                  0.995603         66.666667          99.560260  \n",
      "326                  0.736724         49.000000          73.672358  \n",
      "327                  0.995603         66.666667          99.560260  \n",
      "328                  0.199052         13.000000          19.905176  \n",
      "329                  0.268750         17.666667          26.874996  \n",
      "\n",
      "[330 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = mo_gym.make('mo-circle-env-v0', render_mode='rgb_array')\n",
    "env.unwrapped.configure({\n",
    "    \"screen_width\": 500,\n",
    "    \"screen_height\": 500,\n",
    "    \"observation\": {\n",
    "        \"type\": \"MultiAgentObservation\",\n",
    "        \"observation_config\": {\n",
    "            \"type\": \"Kinematics\",\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "linear_agent = MO_DQN.MO_DQN(env, num_objectives=2, seed=11, observation_space_shape=obs[0].shape, replay_buffer_size=1000, batch_ratio=0.2,\n",
    "                      objective_names=[\"speed_reward\", \"energy_reward\"])\n",
    "linear_agent.train(200_000, epsilon_start=0.1, epsilon_end=0.1, inv_optimisation_frequency=1)\n",
    "\n",
    "df = linear_agent.evaluate(hv_reference_point=np.ndarray([0,0]), seed=11)\n",
    "print(df)\n",
    "df.to_csv(\"data/linear_scalarisation_eval.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chebyshev Scalarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iterations:   0%|          | 0/200000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iterations: 100%|██████████| 200000/200000 [29:21<00:00, 113.51it/s]\n",
      "/home/franz/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.config to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.config` for environment variables or `env.get_wrapper_attr('config')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "Weight tuple: 100%|██████████| 66/66 [04:07<00:00,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     repetition_number  weight_index weight_tuple  normalised_speed_reward  \\\n",
      "0                    0             0   [0.0, 1.0]                 0.030000   \n",
      "1                    1             0   [0.0, 1.0]                 0.666667   \n",
      "2                    2             0   [0.0, 1.0]                 0.666667   \n",
      "3                    3             0   [0.0, 1.0]                 0.536667   \n",
      "4                    4             0   [0.0, 1.0]                -0.003333   \n",
      "..                 ...           ...          ...                      ...   \n",
      "325                  0            65   [1.0, 0.0]                 0.003333   \n",
      "326                  1            65   [1.0, 0.0]                 0.003333   \n",
      "327                  2            65   [1.0, 0.0]                 0.003333   \n",
      "328                  3            65   [1.0, 0.0]                 0.003333   \n",
      "329                  4            65   [1.0, 0.0]                 0.003333   \n",
      "\n",
      "     normalised_energy_reward  raw_speed_reward  raw_energy_reward  \n",
      "0                    0.049698          3.000000           4.969849  \n",
      "1                    0.995603         66.666667          99.560260  \n",
      "2                    0.995603         66.666667          99.560260  \n",
      "3                    0.806422         53.666667          80.642178  \n",
      "4                    0.000000         -0.333333           0.000000  \n",
      "..                        ...               ...                ...  \n",
      "325                  1.000000          0.333333         100.000000  \n",
      "326                  1.000000          0.333333         100.000000  \n",
      "327                  1.000000          0.333333         100.000000  \n",
      "328                  1.000000          0.333333         100.000000  \n",
      "329                  1.000000          0.333333         100.000000  \n",
      "\n",
      "[330 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = mo_gym.make('mo-circle-env-v0', render_mode='rgb_array')\n",
    "env.unwrapped.configure({\n",
    "    \"screen_width\": 500,\n",
    "    \"screen_height\": 500,\n",
    "    \"observation\": {\n",
    "        \"type\": \"MultiAgentObservation\",\n",
    "        \"observation_config\": {\n",
    "            \"type\": \"Kinematics\",\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "obs, info = env.reset()\n",
    "scal_arguments = [torch.tensor([-float(\"inf\"),-float(\"inf\")]), 0.1] #initial utopian and threshold value\n",
    "cheb_agent = MO_DQN.MO_DQN(env, num_objectives=2, seed=11, observation_space_shape=obs[0].shape, replay_buffer_size=1000, batch_ratio=0.2,\n",
    "                      objective_names=[\"speed_reward\", \"energy_reward\"], scalarisation_method=ChebyshevScalarisation, scalarisation_argument_list=scal_arguments)\n",
    "cheb_agent.train(200_000, epsilon_start=0.1, epsilon_end=0.1, inv_optimisation_frequency=1)\n",
    "\n",
    "df = cheb_agent.evaluate(hv_reference_point=np.ndarray([0,0]), seed=11)\n",
    "print(df)\n",
    "df.to_csv(\"data/chebyshev_scalarisation_eval.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
