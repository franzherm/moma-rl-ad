{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import mo_gymnasium as mo_gym\n",
    "import numpy as np\n",
    "from src import MO_DQN\n",
    "from src.utils import ChebyshevScalarisation\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Scalarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franz/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.config to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.config` for environment variables or `env.get_wrapper_attr('config')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "Weight tuple: 100%|██████████| 66/66 [02:25<00:00,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     repetition_number  weight_index weight_tuple  normalised_speed_reward  \\\n",
      "0                    0             0   [0.0, 1.0]                 0.616667   \n",
      "1                    1             0   [0.0, 1.0]                 0.616667   \n",
      "2                    2             0   [0.0, 1.0]                 0.501833   \n",
      "3                    3             0   [0.0, 1.0]                 0.616667   \n",
      "4                    4             0   [0.0, 1.0]                 0.415500   \n",
      "..                 ...           ...          ...                      ...   \n",
      "325                  0            65   [1.0, 0.0]                 0.373000   \n",
      "326                  1            65   [1.0, 0.0]                 0.388000   \n",
      "327                  2            65   [1.0, 0.0]                 0.329000   \n",
      "328                  3            65   [1.0, 0.0]                 0.991167   \n",
      "329                  4            65   [1.0, 0.0]                 0.995500   \n",
      "\n",
      "     normalised_energy_reward  raw_speed_reward  raw_energy_reward  \n",
      "0                    0.945603         61.666667          94.560260  \n",
      "1                    0.945603         61.666667          94.560260  \n",
      "2                    0.774879         50.183333          77.487866  \n",
      "3                    0.945603         61.666667          94.560260  \n",
      "4                    0.642482         41.550000          64.248227  \n",
      "..                        ...               ...                ...  \n",
      "325                  0.368636         37.300000          36.863597  \n",
      "326                  0.383536         38.800000          38.353589  \n",
      "327                  0.325136         32.900000          32.513638  \n",
      "328                  0.983709         99.116667          98.370922  \n",
      "329                  0.984931         99.550000          98.493097  \n",
      "\n",
      "[330 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = mo_gym.make('mo-circle-env-v0', render_mode='rgb_array')\n",
    "env.unwrapped.configure({\n",
    "    \"screen_width\": 500,\n",
    "    \"screen_height\": 500,\n",
    "    \"observation\": {\n",
    "        \"type\": \"MultiAgentObservation\",\n",
    "        \"observation_config\": {\n",
    "            \"type\": \"Kinematics\",\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "linear_agent = MO_DQN.MO_DQN(env, num_objectives=2, seed=11, observation_space_shape=obs[0].shape, replay_buffer_size=1000, batch_ratio=0.2,\n",
    "                      objective_names=[\"speed_reward\", \"energy_reward\"])\n",
    "#linear_agent.train(200_000, epsilon_start=0.1, epsilon_end=0.1, inv_optimisation_frequency=1)\n",
    "\n",
    "df = linear_agent.evaluate(hv_reference_point=np.ndarray([0,0]), seed=11)\n",
    "print(df)\n",
    "df.to_csv(\"data/linear_scalarisation_eval.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chebyshev Scalarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iterations:   0%|          | 739/200000 [00:07<32:12, 103.10it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m scal_arguments \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m),\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)]), \u001b[38;5;241m0.1\u001b[39m] \u001b[38;5;66;03m#initial utopian and threshold value\u001b[39;00m\n\u001b[1;32m     15\u001b[0m cheb_agent \u001b[38;5;241m=\u001b[39m MO_DQN\u001b[38;5;241m.\u001b[39mMO_DQN(env, num_objectives\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m, observation_space_shape\u001b[38;5;241m=\u001b[39mobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, replay_buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, batch_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     16\u001b[0m                       objective_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeed_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menergy_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m], scalarisation_method\u001b[38;5;241m=\u001b[39mChebyshevScalarisation, scalarisation_argument_list\u001b[38;5;241m=\u001b[39mscal_arguments)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mcheb_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_optimisation_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m df \u001b[38;5;241m=\u001b[39m cheb_agent\u001b[38;5;241m.\u001b[39mevaluate(hv_reference_point\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mndarray([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]), seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/src/MO_DQN.py:131\u001b[0m, in \u001b[0;36mMO_DQN.train\u001b[0;34m(self, num_iterations, inv_optimisation_frequency, inv_target_update_frequency, gamma, epsilon_start, epsilon_end)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs, eps_greedy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce_epsilon(num_iterations, epsilon_start, epsilon_end) \u001b[38;5;66;03m#linearly reduce the value of epsilon\u001b[39;00m\n\u001b[1;32m    125\u001b[0m (\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_obs,\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward,\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminated,\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncated,\n\u001b[1;32m    130\u001b[0m     info,\n\u001b[0;32m--> 131\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#accumulate episode reward\u001b[39;00m\n\u001b[1;32m    133\u001b[0m accumulated_rewards \u001b[38;5;241m=\u001b[39m accumulated_rewards \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/highway_env/envs/common/abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[1;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/highway_env/envs/common/abstract.py:258\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mact()\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimulation_frequency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/highway_env/road/road.py:333\u001b[0m, in \u001b[0;36mRoad.step\u001b[0;34m(self, dt)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03mStep the dynamics of each entity on the road.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m:param dt: timestep [s]\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[0;32m--> 333\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles):\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/highway_env/vehicle/kinematics.py:133\u001b[0m, in \u001b[0;36mVehicle.step\u001b[0;34m(self, dt)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheading \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msin(beta) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLENGTH \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m dt\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m dt\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/highway_env/vehicle/kinematics.py:148\u001b[0m, in \u001b[0;36mVehicle.on_state_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_state_update\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad:\n\u001b[0;32m--> 148\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_closest_lane_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheading\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mget_lane(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index)\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mrecord_history:\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/highway_env/road/road.py:61\u001b[0m, in \u001b[0;36mRoadNetwork.get_closest_lane_index\u001b[0;34m(self, position, heading)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _to, lanes \u001b[38;5;129;01min\u001b[39;00m to_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _id, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lanes):\n\u001b[0;32m---> 61\u001b[0m             distances\u001b[38;5;241m.\u001b[39mappend(\u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance_with_heading\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheading\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     62\u001b[0m             indexes\u001b[38;5;241m.\u001b[39mappend((_from, _to, _id))\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexes[\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmin(distances))]\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/highway_env/road/lane.py:125\u001b[0m, in \u001b[0;36mAbstractLane.distance_with_heading\u001b[0;34m(self, position, heading, heading_weight)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m heading \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance(position)\n\u001b[0;32m--> 125\u001b[0m s, r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m angle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_angle(heading, s))\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(r) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(s \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m \u001b[38;5;241m-\u001b[39m s, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m heading_weight\u001b[38;5;241m*\u001b[39mangle\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/highway_env/road/lane.py:317\u001b[0m, in \u001b[0;36mCircularLane.local_coordinates\u001b[0;34m(self, position)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlocal_coordinates\u001b[39m(\u001b[38;5;28mself\u001b[39m, position: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    316\u001b[0m     delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter\n\u001b[0;32m--> 317\u001b[0m     phi \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marctan2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     phi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_phase \u001b[38;5;241m+\u001b[39m utils\u001b[38;5;241m.\u001b[39mwrap_to_pi(phi \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_phase)\n\u001b[1;32m    319\u001b[0m     r \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(delta)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = mo_gym.make('mo-circle-env-v0', render_mode='rgb_array')\n",
    "env.unwrapped.configure({\n",
    "    \"screen_width\": 500,\n",
    "    \"screen_height\": 500,\n",
    "    \"observation\": {\n",
    "        \"type\": \"MultiAgentObservation\",\n",
    "        \"observation_config\": {\n",
    "            \"type\": \"Kinematics\",\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "obs, info = env.reset()\n",
    "scal_arguments = [torch.tensor([-float(\"inf\"),-float(\"inf\")]), 0.1] #initial utopian and threshold value\n",
    "cheb_agent = MO_DQN.MO_DQN(env, num_objectives=2, seed=11, observation_space_shape=obs[0].shape, replay_buffer_size=1000, batch_ratio=0.2,\n",
    "                      objective_names=[\"speed_reward\", \"energy_reward\"], scalarisation_method=ChebyshevScalarisation, scalarisation_argument_list=scal_arguments)\n",
    "cheb_agent.train(200_000, epsilon_start=0.1, epsilon_end=0.1, inv_optimisation_frequency=1)\n",
    "\n",
    "df = cheb_agent.evaluate(hv_reference_point=np.ndarray([0,0]), seed=11)\n",
    "print(df)\n",
    "df.to_csv(\"data/chebyshev_scalarisation_eval.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
