{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import mo_gymnasium as mo_gym\n",
    "import numpy as np\n",
    "from src import MO_DQN\n",
    "from src.utils import ChebyshevScalarisation\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Scalarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations: 100%|██████████| 200000/200000 [36:49<00:00, 90.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      episode  speed_reward  energy_reward\n",
      "0           0      8.783333      15.362052\n",
      "1           1      5.050000       9.787058\n",
      "2           2      8.683333      15.262052\n",
      "3           3      8.633333      15.212052\n",
      "4           4      0.683333       1.932760\n",
      "...       ...           ...            ...\n",
      "2598     2598     13.133333      19.712052\n",
      "2599     2599     13.033333      19.612052\n",
      "2600     2600     13.283333      19.862052\n",
      "2601     2601     13.133333      19.712052\n",
      "2602     2602      0.866667       1.984484\n",
      "\n",
      "[2603 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = mo_gym.make('mo-circle-env-v0', render_mode='rgb_array')\n",
    "env.unwrapped.configure({\n",
    "    \"screen_width\": 500,\n",
    "    \"screen_height\": 500,\n",
    "    \"observation\": {\n",
    "        \"type\": \"MultiAgentObservation\",\n",
    "        \"observation_config\": {\n",
    "            \"type\": \"Kinematics\",\n",
    "        }\n",
    "    }\n",
    "})\n",
    "env.unwrapped.configure({\n",
    "    \"manual_control\": True\n",
    "})\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "linear_agent = MO_DQN.MO_DQN(env, num_objectives=2, seed=11, observation_space_shape=obs[0].shape, replay_buffer_size=1000, batch_ratio=0.2,\n",
    "                      objective_names=[\"speed_reward\", \"energy_reward\"])\n",
    "df = linear_agent.train(200_000, epsilon_start=0.1, epsilon_end=0.1, inv_optimisation_frequency=1)\n",
    "print(df)\n",
    "df.to_csv(\"data/linear_scalarisation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chebyshev Scalarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations: 100%|██████████| 150000/150000 [27:01<00:00, 92.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      episode  speed_reward  energy_reward\n",
      "0           0      5.083333       8.109478\n",
      "1           1     12.133333      18.712052\n",
      "2           2      8.583333      15.162052\n",
      "3           3      2.733333       4.574999\n",
      "4           4     13.283333      19.862052\n",
      "...       ...           ...            ...\n",
      "1960     1960      0.800000       2.181035\n",
      "1961     1961      8.783333      15.362052\n",
      "1962     1962      6.083333      11.478436\n",
      "1963     1963      8.483333      15.062052\n",
      "1964     1964      8.433333      15.012052\n",
      "\n",
      "[1965 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = mo_gym.make('mo-circle-env-v0', render_mode='rgb_array')\n",
    "env.unwrapped.configure({\n",
    "    \"screen_width\": 500,\n",
    "    \"screen_height\": 500,\n",
    "    \"observation\": {\n",
    "        \"type\": \"MultiAgentObservation\",\n",
    "        \"observation_config\": {\n",
    "            \"type\": \"Kinematics\",\n",
    "        }\n",
    "    }\n",
    "})\n",
    "env.unwrapped.configure({\n",
    "    \"manual_control\": True\n",
    "})\n",
    "\n",
    "obs, info = env.reset()\n",
    "scal_arguments = [torch.tensor([-float(\"inf\"),-float(\"inf\")]), 0.1] #initial utopian and threshold value\n",
    "linear_agent = MO_DQN.MO_DQN(env, num_objectives=2, seed=11, observation_space_shape=obs[0].shape, replay_buffer_size=1000, batch_ratio=0.2,\n",
    "                      objective_names=[\"speed_reward\", \"energy_reward\"], scalarisation_method=ChebyshevScalarisation, scalarisation_argument_list=scal_arguments)\n",
    "df = linear_agent.train(150_000, epsilon_start=0.1, epsilon_end=0.1, inv_optimisation_frequency=1)\n",
    "print(df)\n",
    "df.to_csv(\"data/chebyshev_scalarisation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
