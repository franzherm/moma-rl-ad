{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.MOMA_DQN import MOMA_DQN\n",
    "import mo_gymnasium as mo_gym\n",
    "from src.gridsearch import gridsearch\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Weight tuple: 100%|██████████| 10/10 [00:25<00:00,  2.54s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiled modules for significant speedup can not be used!\n",
      "https://pymoo.org/installation.html#installation\n",
      "\n",
      "To disable this warning:\n",
      "from pymoo.config import Config\n",
      "Config.warnings['not_compiled'] = False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Weight tuple: 100%|██████████| 10/10 [00:12<00:00,  1.28s/it]\n",
      "\n",
      "\n",
      "Training episodes:  58%|█████▊    | 58/100 [00:51<00:37,  1.13it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 67\u001b[0m\n\u001b[1;32m     41\u001b[0m run_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m:  [env_config],\n\u001b[1;32m     43\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     },\n\u001b[1;32m     63\u001b[0m }\n\u001b[1;32m     66\u001b[0m env \u001b[38;5;241m=\u001b[39m mo_gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoma-highway-env-v0\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m \u001b[43mgridsearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMOMA_DQN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/moma_highway_test/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmoma_highway\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/src/gridsearch.py:118\u001b[0m, in \u001b[0;36mgridsearch\u001b[0;34m(algorithm, env, run_config, seed, csv_file_path, experiment_name)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     agent \u001b[38;5;241m=\u001b[39m algorithm(env \u001b[38;5;241m=\u001b[39m env, num_objectives \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, seed \u001b[38;5;241m=\u001b[39m seed, observation_space_shape \u001b[38;5;241m=\u001b[39m obs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, num_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, objective_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeed_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menergy_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparameters)\n\u001b[0;32m--> 118\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrun_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_network(csv_file_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_config_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_config_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_exp\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m#run a final evaluation using the trained agent\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/src/MOMA_DQN.py:182\u001b[0m, in \u001b[0;36mMOMA_DQN.train\u001b[0;34m(self, num_episodes, inv_optimisation_frequency, inv_target_update_frequency, gamma, epsilon_start, epsilon_end, epsilon_end_time, num_evaluations, eval_seed)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminated \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncated):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs, eps_greedy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m     (\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_obs,\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards,\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminated,\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncated,\n\u001b[1;32m    181\u001b[0m         info,\n\u001b[0;32m--> 182\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrashed \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrashed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m     vehicle_obj_weights \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvehicle_objective_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/highway_env/envs/common/abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[1;32m    238\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/highway_env/envs/common/abstract.py:257\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_control\u001b[39m\u001b[38;5;124m\"\u001b[39m] \\\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/highway_env/road/road.py:324\u001b[0m, in \u001b[0;36mRoad.act\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decide the actions of each entity on the road.\"\"\"\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[0;32m--> 324\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/highway_env/vehicle/behavior.py:95\u001b[0m, in \u001b[0;36mIDMVehicle.act\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_road()\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_lane_change:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchange_lane_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteering_control(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_lane_index)\n\u001b[1;32m     97\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE)\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/highway_env/vehicle/behavior.py:214\u001b[0m, in \u001b[0;36mIDMVehicle.change_lane_policy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Does the MOBIL model recommend a lane change?\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmobil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlane_index\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_lane_index \u001b[38;5;241m=\u001b[39m lane_index\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/highway_env/vehicle/behavior.py:248\u001b[0m, in \u001b[0;36mIDMVehicle.mobil\u001b[0;34m(self, lane_index)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Is there an acceleration advantage for me and/or my followers to change lane?\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 248\u001b[0m     self_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macceleration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mego_vehicle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfront_vehicle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mold_preceding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     old_following_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macceleration(ego_vehicle\u001b[38;5;241m=\u001b[39mold_following, front_vehicle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    250\u001b[0m     old_following_pred_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macceleration(ego_vehicle\u001b[38;5;241m=\u001b[39mold_following, front_vehicle\u001b[38;5;241m=\u001b[39mold_preceding)\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/highway_env/vehicle/behavior.py:155\u001b[0m, in \u001b[0;36mIDMVehicle.acceleration\u001b[0;34m(self, ego_vehicle, front_vehicle, rear_vehicle)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m front_vehicle:\n\u001b[1;32m    153\u001b[0m     d \u001b[38;5;241m=\u001b[39m ego_vehicle\u001b[38;5;241m.\u001b[39mlane_distance_to(front_vehicle)\n\u001b[1;32m    154\u001b[0m     acceleration \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCOMFORT_ACC_MAX \u001b[38;5;241m*\u001b[39m \\\n\u001b[0;32m--> 155\u001b[0m         np\u001b[38;5;241m.\u001b[39mpower(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesired_gap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mego_vehicle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfront_vehicle\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m utils\u001b[38;5;241m.\u001b[39mnot_zero(d), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m acceleration\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/highway_env/vehicle/behavior.py:170\u001b[0m, in \u001b[0;36mIDMVehicle.desired_gap\u001b[0;34m(self, ego_vehicle, front_vehicle, projected)\u001b[0m\n\u001b[1;32m    168\u001b[0m tau \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTIME_WANTED\n\u001b[1;32m    169\u001b[0m ab \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCOMFORT_ACC_MAX \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCOMFORT_ACC_MIN\n\u001b[0;32m--> 170\u001b[0m dv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(ego_vehicle\u001b[38;5;241m.\u001b[39mvelocity \u001b[38;5;241m-\u001b[39m front_vehicle\u001b[38;5;241m.\u001b[39mvelocity, \u001b[43mego_vehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m projected \\\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ego_vehicle\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m-\u001b[39m front_vehicle\u001b[38;5;241m.\u001b[39mspeed\n\u001b[1;32m    172\u001b[0m d_star \u001b[38;5;241m=\u001b[39m d0 \u001b[38;5;241m+\u001b[39m ego_vehicle\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m*\u001b[39m tau \u001b[38;5;241m+\u001b[39m ego_vehicle\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m*\u001b[39m dv \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(ab))\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m d_star\n",
      "File \u001b[0;32m~/Documents/Master_Project/moma-rl-ad/venv/lib/python3.12/site-packages/highway_env/vehicle/objects.py:129\u001b[0m, in \u001b[0;36mRoadObject.direction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m             d[key] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m origin_dict[key]\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m d\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdirection\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mcos(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheading), np\u001b[38;5;241m.\u001b[39msin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheading)])\n\u001b[1;32m    133\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvelocity\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#final environment config used for experiments\n",
    "env_config = {\n",
    "    \"screen_width\": 500,\n",
    "    \"screen_height\": 500,\n",
    "    \"vehicles_count\": 10,\n",
    "    \"controlled_vehicles\": 2,\n",
    "\n",
    "    \"action\": {\n",
    "        \"type\": \"MultiAgentAction\",\n",
    "        \"action_config\": {\n",
    "            \"type\": \"DiscreteMetaAction\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# run_config = {\n",
    "#     \"env\":  [env_config],\n",
    "\n",
    "#     \"init\": {\n",
    "#          \"replay_buffer_size\": [1000],\n",
    "#          \"batch_ratio\" : [0.2],\n",
    "#          \"reward_structure\" : [\"ego_reward\", \"mean_reward\"],\n",
    "#          \"observation_space_name\" : [\"Kinematics\", \"OccupancyGrid\"],\n",
    "#     },\n",
    "#     \"train\": {\n",
    "#          \"gamma\": 0.9,\n",
    "#          \"num_episodes\" : 25_000,\n",
    "#          \"inv_target_update_frequency\": 10,\n",
    "#          \"epsilon_start\": 0.9,\n",
    "#          \"epsilon_end\": 0,\n",
    "#          \"epsilon_end_time\": 0.75,\n",
    "#     },\n",
    "#     \"eval\": {\n",
    "#         \"num_repetitions\": 20,\n",
    "#         \"num_points\": 20,\n",
    "#         \"num_evaluations\": 10,\n",
    "#     },\n",
    "# }\n",
    "\n",
    "#test run config for generating example data\n",
    "run_config = {\n",
    "    \"env\":  [env_config],\n",
    "\n",
    "    \"init\": {\n",
    "         \"replay_buffer_size\": [1000],\n",
    "         \"batch_ratio\" : [0.2],\n",
    "         \"reward_structure\" : [\"ego_reward\", \"mean_reward\"],\n",
    "         \"observation_space_name\" : [\"Kinematics\", \"OccupancyGrid\"],\n",
    "    },\n",
    "    \"train\": {\n",
    "         \"gamma\": 0.9,\n",
    "         \"num_episodes\" : 100,\n",
    "         \"inv_target_update_frequency\": 10,\n",
    "         \"epsilon_start\": 0.9,\n",
    "         \"epsilon_end\": 0,\n",
    "         \"epsilon_end_time\": 0.75,\n",
    "         \"num_evaluations\": 2\n",
    "    },\n",
    "    \"eval\": {\n",
    "        \"num_repetitions\": 2,\n",
    "        \"num_points\": 3,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "env = mo_gym.make('moma-highway-env-v0', render_mode='rgb_array')\n",
    "gridsearch(MOMA_DQN, env, run_config, 11, csv_file_path=\"data/moma_highway_test/\", experiment_name=\"moma_highway\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Unnamed: 0              400 non-null    int64  \n",
      " 1   episode                 323 non-null    float64\n",
      " 2   loss                    323 non-null    float64\n",
      " 3   hypervolume             8 non-null      float64\n",
      " 4   env_config_id           400 non-null    int64  \n",
      " 5   experiment_id           400 non-null    int64  \n",
      " 6   replay_buffer_size      400 non-null    int64  \n",
      " 7   batch_ratio             400 non-null    float64\n",
      " 8   reward_structure        400 non-null    object \n",
      " 9   observation_space_name  400 non-null    object \n",
      "dtypes: float64(4), int64(4), object(2)\n",
      "memory usage: 31.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>episode</th>\n",
       "      <th>loss</th>\n",
       "      <th>hypervolume</th>\n",
       "      <th>env_config_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>replay_buffer_size</th>\n",
       "      <th>batch_ratio</th>\n",
       "      <th>reward_structure</th>\n",
       "      <th>observation_space_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.279336</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>ego_reward</td>\n",
       "      <td>Kinematics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.180082</td>\n",
       "      <td>0.292104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>ego_reward</td>\n",
       "      <td>Kinematics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.155550</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>ego_reward</td>\n",
       "      <td>OccupancyGrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>50</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.189552</td>\n",
       "      <td>0.255295</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>ego_reward</td>\n",
       "      <td>OccupancyGrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.150507</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>mean_reward</td>\n",
       "      <td>Kinematics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>50</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.102636</td>\n",
       "      <td>0.152172</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>mean_reward</td>\n",
       "      <td>Kinematics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.154508</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>mean_reward</td>\n",
       "      <td>OccupancyGrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>50</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.120965</td>\n",
       "      <td>0.077062</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>mean_reward</td>\n",
       "      <td>OccupancyGrid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  episode      loss  hypervolume  env_config_id  experiment_id  \\\n",
       "0             0      NaN       NaN     0.279336              0              0   \n",
       "50           50     50.0  0.180082     0.292104              0              0   \n",
       "100           0      NaN       NaN     0.155550              0              1   \n",
       "150          50     50.0  0.189552     0.255295              0              1   \n",
       "200           0      NaN       NaN     0.150507              0              2   \n",
       "250          50     50.0  0.102636     0.152172              0              2   \n",
       "300           0      NaN       NaN     0.154508              0              3   \n",
       "350          50     50.0  0.120965     0.077062              0              3   \n",
       "\n",
       "     replay_buffer_size  batch_ratio reward_structure observation_space_name  \n",
       "0                  1000          0.2       ego_reward             Kinematics  \n",
       "50                 1000          0.2       ego_reward             Kinematics  \n",
       "100                1000          0.2       ego_reward          OccupancyGrid  \n",
       "150                1000          0.2       ego_reward          OccupancyGrid  \n",
       "200                1000          0.2      mean_reward             Kinematics  \n",
       "250                1000          0.2      mean_reward             Kinematics  \n",
       "300                1000          0.2      mean_reward          OccupancyGrid  \n",
       "350                1000          0.2      mean_reward          OccupancyGrid  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load the data\n",
    "df_complete = pd.read_csv(\"data/moma_highway_test/moma_highway_merged_loss.csv\")\n",
    "display(df_complete.info())\n",
    "display(df_complete[~df_complete[\"hypervolume\"].isna()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
